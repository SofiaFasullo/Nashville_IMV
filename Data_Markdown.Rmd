---
title: "Nasville Music Venues Research"
author: "Sofia Fasullo"
date: "2023-03-21"
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

Start by loading in packages
```{r}
library(tidyverse)
library(sf)
#library(RSocrata)
library(tidycensus)
#library(tigris)
#library(viridis)
#library(viridisLite)
library(mapview)
library(dplyr)
#library(foreign)
#library(tidygeocoder)
```
Load venue datasets
```{r}
## load venue data (last updated: 04/16/2023)

venues = read.csv("~/GitHub/Nashville_IMV/data/venue_tables/venue_table_filled_4_16.csv") %>% st_as_sf( coords = c("x","y"), remove = FALSE,crs= 4269)

## venue APN data (last updated: 04/16/2023) - use this dataset as a "key" for all tabular joins
venue_admin_key_table = read.csv("~/GitHub/Nashville_IMV/data/venue_tables/simple_table_4_16.csv")

## detailed venue table with survey results (last updated: 07/03/2023)
venue_attrib = read.csv("~/GitHub/Nashville_IMV/data/venue_tables/venue_table_detailed_7_3.csv")

## data dictionary for detailed venue table
data_dictionary_load = read.csv("~/Github/Nashville_IMV/data/venue_tables/data_dictionary_venue_detailed_7_3.csv", header=F)
data_dictionary_view = t(data_dictionary_load) %>% as.data.frame() %>% mutate(col_name=V1,col_definition=V2,survey_question=V3) %>% select(-c(V1,V2,V3))
#view(data_dictionary_view)

```


NOTE: possiblility to streamline this system in the future by reading .R files into this markdown rather than having all this code re-written

#Joining Census Data to Parent Geography

- Use *~/Github/Nashville_IMV/R/clean_data.R* clean the music venue database so that it is in a format compatible to join with other data sets. 

- Then use the code below to inspect and visualize this information.

```{r}
## Sofia Fasullo's API key - replace with your own, get at https://api.census.gov/data/key_signup.html
API_key = "c03e04624fa724c446d835b3c313e3f11aa54c2d" 
census_api_key(API_key, overwrite = TRUE)

## Look through existing acs variables and choose a few to examine (commented out for now)
#acs_variable_list.2020 <- load_variables(2020,"acs5")

## Set up a list of variables to grab
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B06011_001E") # Median income in past 12 months

## Grab block-level for Davidson County: 2020 and 2010
acs2020 <- get_acs(geography = "tract",
                   year = 2020,
                   variables = acs_vars,
                   geometry = TRUE,
                   state = "TN",
                   county = "Davidson",
                   output = "wide") %>%
  select(GEOID, NAME, acs_vars) %>%
  rename(total_pop_est = B01001_001E,
         median_income_est = B06011_001E) %>%
  mutate(year = 2020)

acs2010 <- get_acs(geography = "tract",
                   year = 2010,
                   variables = acs_vars,
                   geometry = TRUE,
                   state = "TN",
                   county = "Davidson",
                   output = "wide") %>%
  select(GEOID, NAME, acs_vars) %>%
  rename(total_pop_est = B01001_001E,
         median_income_est = B06011_001E) %>%
  mutate(year = 2010)

## combining tables in "long data" form - not used currently
#full_table <- rbind(acs2010, acs2020)
```

```{r}
## loading "parent" geography: city council districts (the match up to census block boundaries and )
council_districts = st_read("https://data.nashville.gov/resource/iw7r-m8qr.geojson") %>% select(council_district, geometry) %>% st_transform(crs=4269)
```

Idea is 2 different "big" tables: one organized by census tract and one organized by venue (for ease of different access)

```{r}
## create new table of census acs data to manipulate
census10 = acs2010
census10[is.na(census10)] <- 0 #allow for calculations to be made

## calculate new columns (in this case, area, population density, and preparation to calculate average median income)
census10$area10 = st_area(census10)
census10 = census10 %>% mutate(pop_dens10 = total_pop_est/area10, med_incxpop = median_income_est*total_pop_est)

## join blocks to council districts and aggregate block data by council districts
council10 = st_join(council_districts,census10) %>% group_by(council_district) %>% summarize(tot_pop10 = sum(total_pop_est), avg_med_inc10 = (sum(med_incxpop)/tot_pop10), avg_dens10 = mean(pop_dens10)*2590000) #I made distance in square miles, not meters (that's the *2590000)

## repeat process with 2020 data
census20 = acs2020
census20[is.na(census20)] <- 0
census20$area20 = st_area(census20)
census20 = census20 %>% mutate(pop_dens20 = total_pop_est/area20, med_incxpop = median_income_est*total_pop_est)
council20 = st_join(council_districts,census20) %>% group_by(council_district) %>% summarize(tot_pop20 = sum(total_pop_est), avg_med_inc20 = (sum(med_incxpop)/tot_pop20), avg_dens20 = mean(pop_dens20)*2590000)

## create dataset with council districts containing aggregated census data from both years AND further calculated columns (percent change population, percent change average median income, percent change in population density)
coun_dist = 
  left_join(st_drop_geometry(council10), st_drop_geometry(council20), by="council_district") %>%
  left_join(.,council_districts,by="council_district") %>%
  st_as_sf() %>% 
  mutate(perc_chg_pop = round(((tot_pop20-tot_pop10)/tot_pop10)*100,1), perc_chg_med_inc = round(((avg_med_inc20-avg_med_inc10)/avg_med_inc10)*100,1), perc_chg_dens = round(((avg_dens20-avg_dens10)/avg_dens10)*100,1))
```

```{r}
## aggregating venue data by council district area and creating table of council districts with census acs and venue data
coun_dist_venues = st_join(coun_dist, venues) %>% group_by(council_district) %>% 
  summarize(num_venues = n()) %>% #not sure why NAs are introduced
  left_join(., st_drop_geometry(coun_dist), by="council_district")
```

```{r}
## creating table of venues with census acs data (aggregated by council districts) added on
venues_dist = st_join(venues,coun_dist)
```

## Making Sense of Administrative Data

- Use *~/Github/Nashville_IMV/R/clean_data.R* clean the music venue database so that it is in a format compatible to join with other data sets. 

- Then use the code below to inspect and visualize this information.

The variables in mind:
DAta set: Capacity
Capacity Data (From fire department... what are we missing?)
Data set: Parcels
Zoning
geometry
Centroids (as lat/lon)
Most recent sales value
Sales value by square feet
Sq footage (from property or parcels)
Owner (from parcels)
Data set: Business licenses
License type
Business name (How many are we missing?)

Fire department dataset cleaning
```{r}
#wait for Michael to redirect you to this dataset
```


Parcels dataset cleaning
```{r}
## load administrative data (very time consuming)
parcels = st_read("~/Github/Nashville_IMV/data/metro/Nashville_Parcel_data.shp") %>% st_transform(4269)

venue_all_parcel_data = left_join(venue_admin_key_table,parcels,by= c("address_admin"="PropAddr","APN")) %>% unique()

variables_to_keep = c("name", "address_admin", "APN", "SalePrice", "Acres", "FinishArea", "LUDesc", "Owner", "geometry")

venue_select_parcel_data = venue_all_parcel_data %>% select(variables_to_keep) %>% mutate(Area_sqft = Acres*43560)
#area is of parcel not building so I will not calculate price/sqft just yet
#will get centroids from venues table rather than re-calculating
```

Business Licenses Dataset cleaning
```{r}

```


## Mapping venues
mapping current matched venues by downloading shapefile
```{r}
##joining matched addresses with other features of the venue
pop_venue_db$APN = as.character(pop_venue_db$APN)
venue_partial_geomatched = 
  inner_join(pop_venue_db,APN_join,  by=c("Address_Full", "Address_Number")) %>% 
  st_sf() %>%
  st_transform(st_crs(coun_dist_venues)) %>% 
  mutate(APN = APN.x) %>% 
  select(-c(APN.x,APN.y)) %>% unique()

## tax parcel data is in a polygon of the building layout, but we want venue data in points --> get centroids, split into lat lon for ease later
venue_partial_geomatched$Coordinates_lat = st_coordinates(st_centroid(venue_partial_geomatched$geometry))[,2]
venue_partial_geomatched$Coordinates_lon = st_coordinates(st_centroid(venue_partial_geomatched$geometry))[,1]

## create sf point geometry column
venue_points = 
  venue_partial_geomatched %>% 
  st_drop_geometry() %>% 
  st_as_sf(coords=c("Coordinates_lon", "Coordinates_lat"), crs=st_crs(venue_partial_geomatched)) %>% 
  st_transform(st_crs(coun_dist_venues)) %>% mutate(match_method = "APN")

## identify unmatched venue addresses and prepare for geocoding
unmatched_venues = anti_join(pop_venue_db,APN_join,  by=c("Address_Full", "Address_Number")) %>% inner_join(work_table, by=c("Address_Full", "Address_Number")) %>% unique() %>% select(-c(APN,Coordinates_lat,Coordinates_lon)) %>% na.omit()

## THIS GEOCODING DID NOT VORK
venues_geocoded <- unmatched_venues %>%
  geocode(method = "geocodio", address = APN_Address, lat = lat, long = lon) %>% na.omit() %>% st_as_sf(coords=c("lon", "lat"), crs=st_crs(venue_partial_geomatched)) %>% 
  st_transform(st_crs(coun_dist_venues)) %>% mutate(match_method="geocoded")

## recreate table of all matched and geocoded venues with point geometry
venues_complete = rbind(select(venue_points,c(Venue_name,APN_Address,match_method,geometry)),select(venues_geocoded,c(Venue_name,APN_Address,match_method,geometry)))

## identify venues that are STILL unmatched - not address-matched or geocoded
still_unmatched = anti_join(pop_venue_db,venues_complete,  by="Venue_name") %>% inner_join(work_table, by=c("Address_Full", "Address_Number")) %>% unique() %>% select(c(Venue_name, APN_Address)) %>% mutate(match_method="manual",geometry="") 

## convert matched & geocoded venues to not have spatial properties so they can join to the venues without geometries and a full venue table can be created
to_join = venues_complete %>% as.data.frame()
to_join$geometry = as.character(to_join$geometry)

## join all venues
pop_venue_updated = rbind(to_join,still_unmatched)

## save to csv (ALREADY DONE)
#st_write(pop_venue_updated, "C:/Users/sofia/Documents/UPenn/Nashville/get_to_work.csv")

##  manually add point geometry for the venues without (looked address up on google maps, copied coordinates, formatted)

## upload csv to R
venues_manual = read_csv("C:/Users/sofia/Documents/UPenn/Nashville/Manual.csv") %>% select(Venue_name, APN_Address, match_method, geometry)

## re-convert to sf with point geometry
venues_manual$geometry = st_as_sfc(venues_manual$geometry)
venues_manual = venues_manual %>% st_as_sf()

## write as shapefile to map (ALREADY)
#st_write(venues_manual, "C:/Users/sofia/Documents/UPenn/Nashville/Nashville_Venues_Manual.shp")
#no crs but should be 4269 - complete manually in mapping software

```

```{r}
## venues that I manually put coordinates into
venue_points = st_read("C:/Users/sofia/Documents/UPenn/Nashville/Nashville_Venues_Manual.shp")

## load only necessary columns of administrative tax parcel data for ease of use
admin_short = parcels %>% select("PropAddr","geometry")

## spatial join of manual and APN-address-matched points to tax parcels (APN-addressed-matched venues should result in APN_Address=PropAddr)
spatial_join = st_join(venue_points,admin_short,join=st_intersects) #DOES NOT VORK, DUPLICATE VERTICES
```



```{r}
## plotting current venues on ggplot
#ggplot() +
#  geom_sf(mapping = aes(geometry=geometry, fill=num_venues), data=coun_dist_venues) +
#  geom_sf(mapping = aes(geometry=geometry), data=venue_complete, color="white", fill=NA) +
#  geom_sf(mapping = aes(geometry=geometry), data=venue_points, color="red", size=0.1)
```

## Demographic Analysis

Ideas: 
Demographic Change
Gentrification
 - all this over time
 - income and race, age, industry (QCAW)
wage stagnation
  - profit?
  - QCAW
  - where people travel for work
Diversity in ownership??
- state vs local? crackdown on LGBTQ+ spaces??
